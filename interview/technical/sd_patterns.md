# ğŸ—ï¸ System Design Patterns: Data Engineering

A modular guide to designing scalable, reliable, and maintainable data systems. Includes recognition cues, design patterns, tradeoffs, and annotated templates.



## ğŸ§± Batch vs Streaming Architecture

**Recognize When:**
- â€œDesign a data pipeline for real-time analyticsâ€
- â€œHow would you process large volumes of historical data?â€

**Design Pattern:**

| Use Case     | Batch                          | Streaming                        |
|--------------|--------------------------------|----------------------------------|
| Latency      | Minutes to hours               | Seconds to milliseconds          |
| Tools        | Spark, Airflow, dbt            | Kafka, Flink, Spark Streaming    |
| Storage      | Data Lake, Warehouse           | Kafka topics, real-time stores   |
| Complexity   | Lower operational overhead     | Higher coordination and tuning   |

**Tradeoffs:**
- Streaming offers freshness but increases complexity  
- Batch is simpler and cost-effective for non-real-time needs


## ğŸ§® Data Modeling & Storage

**Recognize When:**
- â€œHow would you store user activity logs?â€
- â€œDesign a schema for a multi-tenant analytics platformâ€

**Design Pattern:**
- Dimensional Modeling (Star/Snowflake)
- Wide Tables for denormalized access
- Hybrid: Semi-denormalized with only essential dimension tables
- Partitioning & Clustering for performance
- Lakehouse Architecture (Delta, Iceberg, Hudi)

**Tradeoffs:**
- Normalized models reduce redundancy but increase join cost  
- Denormalized models improve performance but risk duplication


## ğŸ”„ ETL vs ELT

**Recognize When:**
- â€œDesign a pipeline to ingest and transform data from multiple sourcesâ€

**Design Pattern:**

| Approach | ETL (Transform before Load) | ELT (Transform after Load) |
|----------|-----------------------------|----------------------------|
| Tools    | Informatica, Talend, custom scripts | dbt, Spark SQL, BigQuery |
| Flexibility | Lower (fixed schema upfront) | Higher (schema-on-read)   |
| Storage Cost | Lower (pre-cleaned data) | Higher (raw data retained) |

**Tradeoffs:**
- ELT is preferred in modern cloud-native setups  
- ETL may be better for compliance or legacy systems


## ğŸ§  Orchestration & Dependency Management

**Recognize When:**
- â€œHow do you ensure pipeline reliability?â€
- â€œDesign a system that handles task dependencies and retriesâ€

**Design Pattern:**
- DAGs (Airflow, Prefect)
- Event-Driven Triggers (Kafka, AWS Lambda)
- Retry & Alerting (Exponential backoff, DLQs)

**Tradeoffs:**
- DAGs offer visibility but can become brittle  
- Event-driven systems scale better but are harder to debug


## ğŸ›¡ï¸ Data Quality & Observability

**Recognize When:**
- â€œHow do you detect and handle bad data?â€
- â€œDesign a system to monitor pipeline healthâ€

**Design Pattern:**
- Validation Checks (nulls, ranges, schema drift)
- Data Contracts (schema enforcement)
- Monitoring Tools (Great Expectations, Monte Carlo)

**Tradeoffs:**
- Strict validation prevents bad data but may block ingestion  
- Loose validation allows flexibility but risks silent corruption


## ğŸ§® Scalability & Cost Optimization

**Recognize When:**
- â€œDesign a pipeline that handles billions of events per dayâ€
- â€œHow do you optimize for cost in cloud data processing?â€

**Design Pattern:**
- Partitioning & Bucketing
- Caching & Materialized Views
- Auto-scaling & Spot Instances
- Columnar Storage (Parquet, ORC)

**Tradeoffs:**
- Aggressive optimization may reduce flexibility  
- Over-partitioning can hurt performance


## ğŸ¢ Multi-Tenant Architecture

**Recognize When:**
- â€œDesign a platform for multiple clients or teamsâ€
- â€œHow do you isolate data and compute across tenants?â€

**Design Pattern:**
- Namespace Isolation (schemas, folders, buckets)
- Row-Level Security or Column Masking
- Metadata tagging for tenant lineage
- Resource Quotas and Usage Tracking

**Tradeoffs:**
- Full isolation improves security but increases cost  
- Shared infrastructure reduces overhead but risks noisy neighbors


## ğŸ§¬ Data Lineage & Governance

**Recognize When:**
- â€œHow do you track where data came from?â€
- â€œDesign a system for auditing and complianceâ€

**Design Pattern:**
- Lineage Tracking Tools (OpenLineage, Marquez, Unity Catalog)
- Metadata Propagation via Orchestration
- Versioning & Audit Logs
- Column-Level Lineage for sensitive fields

**Tradeoffs:**
- Deep lineage improves trust but adds metadata overhead  
- Shallow lineage is easier but less informative


## ğŸš¨ Real-Time Alerting Systems

**Recognize When:**
- â€œHow do you detect pipeline failures or anomalies in real time?â€

**Design Pattern:**
- Metric Thresholds (latency, volume, error rate)
- Anomaly Detection (Z-score, time-series models)
- Alert Routing (PagerDuty, Slack, email)
- Dead Letter Queues for failed events

**Tradeoffs:**
- Real-time alerts reduce downtime but risk false positives  
- Batch alerts are simpler but slower to respond


## ğŸ§° Preparation Tips

### ğŸ§  1. Master Core Patterns
- Internalize tradeoffs between batch vs streaming, ETL vs ELT, and modeling strategies
- Use this cheatsheet to map patterns to use cases quickly

### ğŸ“š 2. Build a Design Prompt Bank
- Collect 10â€“15 common system design questions
- Tag each with relevant patterns (e.g., orchestration + lineage)

### ğŸ—£ï¸ 3. Practice Out Loud
- Use whiteboard or markdown to sketch architecture
- Explain decisions, tradeoffs, and failure modes clearly

### ğŸ§­ 4. Prioritize Constraints
- Always ask about scale, latency, SLAs, and data freshness
- Tailor your design to meet specific business goals

### ğŸ§± 5. Layer Your Design
- Start with high-level architecture, then drill into ingestion, storage, processing, and observability
- Use modular blocks: ingestion â†’ transformation â†’ storage â†’ access â†’ monitoring

### ğŸ” 6. Annotate Your Diagrams
- Label components with purpose, tools, and failure handling
- Highlight bottlenecks, retries, and lineage paths

### ğŸ§‘â€ğŸ« 7. Teach It Back
- Practice explaining your design to a junior engineer or non-technical stakeholder
- Focus on clarity, modularity, and real-world applicability

<br>

# ğŸ§­ System Design Promps

A set of reusable prompts and annotated response templates


## ğŸ”„ Prompt: Design a Real-Time Analytics Pipeline

**Use Cases:** Clickstream, IoT, fraud detection  
**Patterns:** Streaming architecture, ELT, observability

**Response Template:**
- Ingestion: Kafka â†’ Spark Structured Streaming  
- Transformation: Stateless + windowed aggregations  
- Storage: Real-time store (e.g., Druid, Pinot)  
- Access: Dashboard or API  
- Monitoring: Lag metrics, schema drift alerts  
- Tradeoffs: Low latency vs complexity, schema evolution


## ğŸ§¬ Prompt: Design a Data Lineage System

**Use Cases:** Compliance, auditing, debugging  
**Patterns:** Metadata propagation, orchestration, governance

**Response Template:**
- Capture lineage via Airflow DAGs or Spark jobs  
- Store metadata in Marquez or Unity Catalog  
- Expose lineage via UI or API  
- Version datasets and transformations  
- Tradeoffs: Metadata overhead vs auditability


## ğŸ¢ Prompt: Design a Multi-Tenant Data Platform

**Use Cases:** SaaS analytics, internal teams  
**Patterns:** Namespace isolation, quotas, row-level security

**Response Template:**
- Isolate tenants via schema or bucket naming  
- Enforce access via IAM + row-level policies  
- Track usage via metadata tagging  
- Monitor noisy neighbor impact  
- Tradeoffs: Isolation vs cost, shared infra vs governance


## ğŸ›¡ï¸ Prompt: Design a Data Quality Framework

**Use Cases:** Ingestion validation, schema enforcement  
**Patterns:** Contracts, validation checks, alerting

**Response Template:**
- Define expectations via Great Expectations or dbt tests  
- Validate on ingestion and transformation  
- Route failures to DLQ or quarantine zone  
- Alert via Slack or PagerDuty  
- Tradeoffs: Strict checks vs ingestion continuity


## ğŸ§® Prompt: Design a Cost-Optimized Batch Pipeline

**Use Cases:** Daily reports, historical backfills  
**Patterns:** Partitioning, columnar formats, spot instances

**Response Template:**
- Ingest via Airflow â†’ Spark  
- Store in Parquet with partitioning  
- Use spot instances and auto-scaling  
- Materialize views for frequent queries  
- Tradeoffs: Cost vs flexibility, latency vs throughput


## ğŸ“ Capacity Estimation Template

Use this template to estimate system load, throughput, and scaling requirements. Ideal for sizing ingestion pipelines, APIs, databases, and storage layers.

### ğŸ§¾ 1. Traffic Profile

**Inputs:**
- Daily Active Users (DAU): `e.g., 1M`
- Peak Concurrent Users: `e.g., 50K`
- Requests per User per Day: `e.g., 20`
- Avg Payload Size: `e.g., 2 KB`

**Derived Metrics:**
- Total Requests/day = `DAU Ã— Requests/user` â†’ `20M req/day`
- Peak QPS = `Concurrent Users Ã— Ops/user Ã· 60 sec` â†’ `~8K QPS`
- Bandwidth/day = `Total Requests Ã— Payload Size` â†’ `~40 GB/day`


### âš™ï¸ 2. Storage Estimation

**Inputs:**
- Event Size: `e.g., 1 KB`
- Retention Period: `e.g., 30 days`
- Daily Volume: `e.g., 100M events/day`

**Derived Metrics:**
- Daily Storage = `100M Ã— 1 KB` â†’ `~100 GB/day`
- Monthly Storage = `100 GB Ã— 30` â†’ `~3 TB`
- Add replication factor (e.g., Ã—3) â†’ `~9 TB`


### ğŸš€ 3. Throughput & Scaling

**Inputs:**
- Target Latency: `e.g., <100 ms`
- Processing Time per Event: `e.g., 10 ms`
- Parallelism: `e.g., 100 threads`

**Derived Metrics:**
- Max Throughput = `Threads Ã— (1 / Processing Time)` â†’ `~10K events/sec`
- Required Partitions (Kafka) = `Daily Volume Ã· Partition Throughput Ã· Seconds/day`  
  â†’ `100M Ã· 10MB/s Ã· 86,400` â†’ `~115 partitions`


### ğŸ›¡ï¸ 4. Reliability & Redundancy

**Checklist:**
- âœ… Replication factor (e.g., Kafka, HDFS)
- âœ… Retry strategy (exponential backoff, DLQ)
- âœ… SLA targets (e.g., 99.9% uptime)
- âœ… Alerting thresholds (latency, error rate)


### ğŸ§  5. Interview Framing Tips

- Always clarify: **read/write ratio**, **peak vs average**, **latency vs throughput**
- Use round numbers for quick math (e.g., 1K req/sec = ~86M/day)
- Justify tradeoffs: cost vs performance, complexity vs reliability
- Layer your estimates: ingestion â†’ processing â†’ storage â†’ access

<br>

# âš™ï¸ Latency & Throughput Values (Back-of-the-Envelope)

A quick-access guide to average response times, bandwidth, and throughput across common system components. Use these estimates for capacity planning, bottleneck analysis, and interview design tradeoffs.


## ğŸ•’ Latency Estimates

| Operation                          | Avg Latency     | Notes                                                  |
|------------------------------------|-----------------|--------------------------------------------------------|
| L1 Cache Access                    | ~0.5 ns         | CPU-level, extremely fast                              |
| RAM Access                         | ~100 ns         | Memory lookup                                          |
| SSD Read                           | ~100 Âµs         | NVMe faster than SATA                                  |
| HDD Read                           | ~10 ms          | Mechanical disk                                        |
| Local Disk I/O (SSD)               | ~0.1â€“1 ms       | Depends on block size and queue depth                  |
| Network (Same Region)              | ~0.5â€“1 ms       | Intra-AZ or VPC                                        |
| Network (Cross Region)             | ~50â€“100 ms      | Depends on geography                                   |
| RDBMS Insert (Single Row)          | ~0.1â€“10 ms      | Depends on indexing, constraints, logging, and ACID    |
| RDBMS Insert (No Index/ACID)       | ~0.1â€“1 ms       | Minimal overhead; often used in ingestion pipelines    |
| RDBMS Query (Indexed)              | ~1â€“10 ms        | Simple SELECT with index                               |
| RDBMS Query (Join/Aggregate)       | ~10â€“100 ms      | Depends on data size and indexing                      |
| RDBMS Fetch (Indexed Row)          | ~0.1â€“1 ms       | Point lookup via primary/unique index                  |
| RDBMS Fetch (Range Scan)           | ~1â€“10 ms        | Depends on index selectivity and row count             |
| Redis GET/SET                      | ~0.2â€“1 ms       | In-memory cache                                        |
| Memcached GET                      | ~0.2â€“0.5 ms     | Slightly faster than Redis due to simpler protocol     |
| Kafka Publish/Consume              | ~2â€“10 ms        | Depends on batch size and partitioning                 |
| API Gateway + Lambda               | ~10â€“100 ms      | Cold start adds latency                                |
| HTTP Request (Internet)            | ~100â€“500 ms     | Includes DNS, TLS, routing                             |
| DynamoDB Read (Strongly Consistent)| ~1â€“10 ms        | Latency varies by partition size and access pattern    |
| DynamoDB Write                     | ~5â€“20 ms        | Includes replication and consistency overhead          |
| gRPC Call (Intra-region)           | ~0.5â€“5 ms       | Binary protocol, lower overhead than HTTP              |
| TLS Handshake                      | ~10â€“100 ms      | One-time cost per connection; reused in keep-alive     |
| Container Startup (Warm)           | ~50â€“200 ms      | Depends on image size and runtime                      |
| Container Startup (Cold)           | ~500 msâ€“5 sec   | Includes image pull and init; varies by orchestration  |

## ğŸŒ Network Latency by Distance (Round-Trip Estimates)

| Distance           | One-Way Delay | Est. RTT (Real-World)  | Notes                                           |
|--------------------|---------------|------------------------|-------------------------------------------------|
| 1 mile             | ~8 Âµs         | ~20â€“50 Âµs              | Negligible; dominated by processing overhead    |
| 100 miles          | ~0.8 ms       | ~2â€“5 ms                | Intra-region, low hop count                     |
| 500 miles          | ~4 ms         | ~10â€“20 ms              | Regional routing, moderate congestion           |
| 1,000 miles        | ~8 ms         | ~20â€“40 ms              | Cross-country RTT (e.g., NY â†” TX)               |
| 3,000 miles        | ~24 ms        | ~50â€“80 ms              | Coast-to-coast RTT (e.g., NY â†” SF)              |
| 6,000+ miles       | ~48 ms        | ~100â€“150 ms            | Transatlantic or transpacific RTT               |
| 10,000+ miles      | ~80 ms        | ~150â€“250 ms            | Global RTT (e.g., US â†” Australia)               |

**Formula (approx):**  
- **Propagation delay** â‰ˆ `Distance / Speed of signal`  
- Fiber optic speed â‰ˆ `200,000 km/s` â†’ ~5 Âµs/km or ~8 Âµs/mile  
- Real-world latency includes routing, queuing, NAT, congestion, and retransmissions


## ğŸ“¦ Throughput & Bandwidth Estimates

| Component                          | Throughput        | Notes                                                |
|------------------------------------|-------------------|------------------------------------------------------|
| Gigabit Ethernet                   | ~125 MB/s         | Theoretical max                                      |
| NVMe SSD                           | ~2â€“5 GB/s         | Sequential read/write                                |
| HDD                                | ~100 MB/s         | Sequential access                                    |
| Kafka Broker                       | ~100 MB/sâ€“1 GB/s  | Depends on partitioning and replication              |
| S3 Upload/Download                 | ~50â€“100 MB/s      | Per connection, can parallelize                      |
| RDBMS Insert (Single Row)          | ~1â€“10K rows/sec   | Varies by indexing, transaction size                 |
| RDBMS Batched Inserts (100â€“1K rows)| ~10Kâ€“100K rows/sec| Reduces round-trips and commit cost                  |
| RDBMS Bulk Insert (COPY, bcp, etc) | ~100Kâ€“1M rows/min | Engine-optimized; bypasses SQL layer                 |
| RDBMS Bulk Fetch (Indexed)         | ~10Kâ€“100K rows/sec| Depends on query plan, buffer size, and network      |
| RDBMS Bulk Insert (Batch)          | ~10Kâ€“1M rows/min  | Batch size and transaction tuning matter             |
| Spark Job (Clustered)              | ~100 MB/sâ€“1 GB/s  | Depends on parallelism and I/O                       |
| Redis Ops                          | ~100Kâ€“500K ops/sec| In-memory, single-threaded                           |
| Redis Pub/Sub                      | ~100K msgs/sec    | Depends on message size and subscriber count         |
| Memcached Ops                      | ~200Kâ€“1M ops/sec  | Lightweight protocol, single-threaded                |
| DynamoDB Ops                       | ~1Kâ€“10K ops/sec   | Scales with partition key distribution               |
| API Gateway                        | ~10Kâ€“100K req/min | Scales horizontally                                  |
| Lambda Concurrency (Default Limit) | ~1K concurrent    | Soft limit; can be raised                            |
| gRPC Streaming                     | ~10â€“100 MB/s      | Efficient for binary payloads over persistent conns. |
| SQS Message Throughput             | ~100â€“300 msg/sec  | Per queue; can scale with batching and parallelism   |
| WebSocket Broadcast                | ~10Kâ€“100K clients | Depends on server and message frequency              |


## ğŸ“ Capacity Planning Tips

- **Rule of Thumb:**  
  - 1 GB RAM â‰ˆ 1M small objects (e.g., Redis keys)  
  - 1 Kafka partition â‰ˆ 10 MB/s throughput  
  - 1 Lambda cold start â‰ˆ 100â€“500 ms latency  
  - 1 RDS vCPU â‰ˆ 100â€“500 queries/sec (simple SELECT)

- **Estimate Load:**  
  - Requests/sec Ã— Avg Payload Size = Bandwidth  
  - Events/day Ã· Batch size = Processing frequency  
  - Concurrent users Ã— Ops/user = Peak throughput

- **Scaling Strategy:**  
  - Use horizontal scaling for stateless services  
  - Use partitioning/sharding for stateful stores  
  - Cache hot paths to reduce latency and load


## ğŸ§  Interview Use Cases

- â€œEstimate how many Kafka partitions you'd need for 1 TB/day ingestionâ€  
- â€œHow would you scale a Redis cache for 1M QPS?â€  
- â€œDesign a system to serve 10K concurrent users with sub-100ms latencyâ€
